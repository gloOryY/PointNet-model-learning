"""

–î–û–û–ë–£–ß–ï–ù–ò–ï –î–õ–Ø TESLA T4 - –í–ï–†–°–ò–Ø –° POINT-WISE SEGMENTATION

–ì–ª–∞–≤–Ω–æ–µ –æ—Ç–ª–∏—á–∏–µ –æ—Ç classification:
- –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç label –¥–ª—è –ö–ê–ñ–î–û–ô —Ç–æ—á–∫–∏ –æ—Ç–¥–µ–ª—å–Ω–æ (–Ω–µ –¥–ª—è –≤—Å–µ–≥–æ –æ–±–ª–∞–∫–∞)
- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ PointNet Segmentation —Å per-point classifier
- Loss –∏ –º–µ—Ç—Ä–∏–∫–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–π –∏–∑ 2048 —Ç–æ—á–µ–∫

–°–æ–≤–º–µ—Å—Ç–∏–º–æ —Å generate_detailed_city_scenes_v2.py
"""

# ============================================================================
# –ë–õ–û–ö 1: –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö –ò –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø
# ============================================================================

import numpy as np
# numpy (np) ‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–º–∏ –º–∞—Å—Å–∏–≤–∞–º–∏

import torch
# torch ‚Äî –≥–ª–∞–≤–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ PyTorch –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏ –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏

import torch.nn as nn
# nn (neural network) ‚Äî –º–æ–¥—É–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π

import torch.nn.functional as F
# F (functional) ‚Äî —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∏ –ø–æ—Ç–µ—Ä—å

from torch.utils.data import Dataset, DataLoader
# Dataset, DataLoader ‚Äî —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö

import glob
# glob ‚Äî –ø–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ –ø–æ –º–∞—Å–∫–∞–º

from tqdm import tqdm
# tqdm ‚Äî –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è

from plyfile import PlyData
# plyfile ‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è PLY —Ñ–∞–π–ª–æ–≤

import time
# time ‚Äî –º–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º

import os
# os ‚Äî —Ä–∞–±–æ—Ç–∞ —Å –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π

print("=" * 80)
print("POINTNET SEGMENTATION - –û–ë–£–ß–ï–ù–ò–ï")
print("=" * 80)

# ============================================================================
# –ë–õ–û–ö 2: –ù–ê–°–¢–†–û–ô–ö–ê –ü–ê–†–ê–ú–ï–¢–†–û–í –û–ë–£–ß–ï–ù–ò–Ø
# ============================================================================

# ========== –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø ==========

BATCH_SIZE = 16
# –ù–û–í–û–ï: –£–º–µ–Ω—å—à–µ–Ω–æ —Å 32 –¥–æ 16, –ø–æ—Ç–æ–º—É —á—Ç–æ segmentation —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏
# –ö–∞–∂–¥–æ–µ –æ–±–ª–∞–∫–æ —Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 2048 labels –≤–º–µ—Å—Ç–æ –æ–¥–Ω–æ–≥–æ

NUM_POINTS = 2048
# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –≤ –æ–±–ª–∞–∫–µ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è PointNet)

NUM_EPOCHS = 20
# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è

LEARNING_RATE = 0.0001
# –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è

VAL_SPLIT = 0.2
# –î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (20%)

USE_MIXED_PRECISION = True
# –°–º–µ—à–∞–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è

NUM_WORKERS = 2
# –ü–æ—Ç–æ–∫–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö

# ========== –ü–£–¢–ò –ö –î–ê–ù–ù–´–ú ==========

MIXED_DATA_FOLDER = "dataset/mixed"
# –ü–∞–ø–∫–∞ —Å PLY-—Ñ–∞–π–ª–∞–º–∏ —Å–æ —Å–º–µ—à–∞–Ω–Ω—ã–º–∏ labels

BASE_MODEL = "best_model_segmentation.pth"
# –ù–û–í–û–ï: –î—Ä—É–≥–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è segmentation –º–æ–¥–µ–ª–∏

# ============================================================================
# –ë–õ–û–ö 3: –í–´–ë–û–† –£–°–¢–†–û–ô–°–¢–í–ê
# ============================================================================

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"\nDevice: {device}")

if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory/(1024**3):.1f} GB")

print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: BATCH={BATCH_SIZE}, WORKERS={NUM_WORKERS}, EPOCHS={NUM_EPOCHS}")

# ============================================================================
# –ë–õ–û–ö 4: –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ê–†–•–ò–¢–ï–ö–¢–£–†–´ TNet
# ============================================================================

class TNet(nn.Module):
    """
    TNet (Transformation Network) ‚Äî –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ç—å –¥–ª—è PointNet
    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —Ç–æ—á–µ–∫ –æ–±–ª–∞–∫–∞
    """

    def __init__(self, k=3):
        """
        Args:
            k (int): —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (3 –¥–ª—è xyz, 64 –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        """
        super(TNet, self).__init__()
        self.k = k

        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏
        self.conv1 = nn.Conv1d(k, 64, 1)
        self.conv2 = nn.Conv1d(64, 128, 1)
        self.conv3 = nn.Conv1d(128, 1024, 1)

        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, k*k)

        # Batch Normalization
        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(1024)
        self.bn4 = nn.BatchNorm1d(512)
        self.bn5 = nn.BatchNorm1d(256)

    def forward(self, x):
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ TNet

        Args:
            x: (batch, k, num_points)

        Returns:
            –ú–∞—Ç—Ä–∏—Ü–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (batch, k, k)
        """
        batch_size = x.size(0)

        # Encoder: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))

        # Global Max Pooling
        x = torch.max(x, 2, keepdim=True)[0]
        x = x.view(batch_size, -1)

        # Decoder: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã
        x = F.relu(self.bn4(self.fc1(x)))
        x = F.relu(self.bn5(self.fc2(x)))
        x = self.fc3(x)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –µ–¥–∏–Ω–∏—á–Ω–æ–π –º–∞—Ç—Ä–∏—Ü–µ–π
        identity = torch.eye(self.k, device=x.device).flatten()
        x = x + identity
        x = x.view(batch_size, self.k, self.k)

        return x

# ============================================================================
# –ë–õ–û–ö 5: –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ê–†–•–ò–¢–ï–ö–¢–£–†–´ POINTNET SEGMENTATION
# ============================================================================

class PointNetSegmentation(nn.Module):
    """
    –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: PointNet –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏

    –ì–ª–∞–≤–Ω–æ–µ –æ—Ç–ª–∏—á–∏–µ –æ—Ç classification:
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç labels –¥–ª—è –ö–ê–ñ–î–û–ô —Ç–æ—á–∫–∏ (batch, num_points)
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - Per-point classifier –≤–º–µ—Å—Ç–æ global classifier
    """

    def __init__(self, num_classes=2):
        """
        Args:
            num_classes: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ (2 –¥–ª—è –∑–¥–∞–Ω–∏–µ/–Ω–µ-–∑–¥–∞–Ω–∏–µ)
        """
        super(PointNetSegmentation, self).__init__()

        # === INPUT TRANSFORM ===
        self.input_transform = TNet(k=3)

        # === –ü–ï–†–í–ê–Ø –ì–†–£–ü–ü–ê –°–í–Å–†–¢–û–ö (–õ–û–ö–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò) ===
        self.conv1 = nn.Conv1d(3, 64, 1)
        self.conv2 = nn.Conv1d(64, 128, 1)
        self.conv3 = nn.Conv1d(128, 128, 1)
        # –ù–û–í–û–ï: –¢—Ä–µ—Ç–∏–π —Å–ª–æ–π —Ç–µ–ø–µ—Ä—å 128 (–Ω–µ 1024), —Ç–∞–∫ –∫–∞–∫ –Ω–∞–º –Ω—É–∂–Ω—ã –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏

        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(128)

        # === FEATURE TRANSFORM ===
        self.feature_transform = TNet(k=64)

        # === –í–¢–û–†–ê–Ø –ì–†–£–ü–ü–ê –°–í–Å–†–¢–û–ö (–ì–õ–£–ë–û–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò) ===
        self.conv4 = nn.Conv1d(128, 512, 1)
        self.conv5 = nn.Conv1d(512, 2048, 1)
        # –ù–û–í–û–ï: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 2048 –¥–ª—è –±–æ–ª–µ–µ –±–æ–≥–∞—Ç—ã—Ö –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

        self.bn4 = nn.BatchNorm1d(512)
        self.bn5 = nn.BatchNorm1d(2048)

        # === PER-POINT SEGMENTATION HEAD ===
        # –ù–û–í–û–ï: –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏
        # –í—Ö–æ–¥: 64 (–ª–æ–∫–∞–ª—å–Ω—ã–µ –ø–æ—Å–ª–µ conv1) + 2048 (–≥–ª–æ–±–∞–ª—å–Ω—ã–µ) = 2112
        self.conv6 = nn.Conv1d(2112, 512, 1)
        self.conv7 = nn.Conv1d(512, 256, 1)
        self.conv8 = nn.Conv1d(256, 128, 1)
        self.conv9 = nn.Conv1d(128, num_classes, 1)
        # –í—ã—Ö–æ–¥: (batch, num_classes, num_points)

        self.bn6 = nn.BatchNorm1d(512)
        self.bn7 = nn.BatchNorm1d(256)
        self.bn8 = nn.BatchNorm1d(128)

        self.dropout = nn.Dropout(p=0.3)

    def forward(self, x):
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ PointNet Segmentation

        Args:
            x: (batch, 3, num_points) ‚Äî –≤—Ö–æ–¥–Ω–æ–µ –æ–±–ª–∞–∫–æ

        Returns:
            tuple:
                - logits: (batch, num_classes, num_points) ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏
                - trans: (batch, 3, 3) ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ input transform
                - trans_feat: (batch, 64, 64) ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ feature transform
        """
        batch_size = x.size(0)
        num_points = x.size(2)

        # === STEP 1: INPUT TRANSFORM ===
        trans = self.input_transform(x)
        x = torch.bmm(trans, x)

        # === STEP 2: –õ–û–ö–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò ===
        x = F.relu(self.bn1(self.conv1(x)))
        # –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è
        local_features = x  # (batch, 64, num_points)

        # === STEP 3: FEATURE TRANSFORM ===
        trans_feat = self.feature_transform(x)
        x = x.transpose(2, 1)
        x = torch.bmm(x, trans_feat)
        x = x.transpose(2, 1)

        # === STEP 4: –ì–õ–£–ë–û–ö–ò–ï –õ–û–ö–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò ===
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))

        # === STEP 5: –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò ===
        x = F.relu(self.bn4(self.conv4(x)))
        x = F.relu(self.bn5(self.conv5(x)))

        # Global Max Pooling: –∞–≥—Ä–µ–≥–∞—Ü–∏—è –≤ –≥–ª–æ–±–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        global_features = torch.max(x, 2, keepdim=True)[0]  # (batch, 2048, 1)

        # –†–∞–∑–º–Ω–æ–∂–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏
        global_features = global_features.repeat(1, 1, num_points)  # (batch, 2048, num_points)

        # === STEP 6: –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –õ–û–ö–ê–õ–¨–ù–´–• –ò –ì–õ–û–ë–ê–õ–¨–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í ===
        # –ö–õ–Æ–ß–ï–í–ê–Ø –ò–î–ï–Ø SEGMENTATION: –∫–∞–∂–¥–∞—è —Ç–æ—á–∫–∞ –ø–æ–ª—É—á–∞–µ—Ç:
        # - –°–≤–æ–∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—á—Ç–æ —ç—Ç–æ –∑–∞ —Ç–æ—á–∫–∞)
        # - –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–≤ –∫–∞–∫–æ–º –æ–±–ª–∞–∫–µ –æ–Ω–∞ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è)
        x = torch.cat([local_features, global_features], dim=1)  # (batch, 2112, num_points)

        # === STEP 7: PER-POINT CLASSIFICATION ===
        x = F.relu(self.bn6(self.conv6(x)))
        x = self.dropout(x)
        x = F.relu(self.bn7(self.conv7(x)))
        x = self.dropout(x)
        x = F.relu(self.bn8(self.conv8(x)))
        x = self.conv9(x)  # (batch, num_classes, num_points)

        # –ù–û–í–û–ï: –ù–µ –ø—Ä–∏–º–µ–Ω—è–µ–º softmax –∑–¥–µ—Å—å, CrossEntropyLoss —Å–¥–µ–ª–∞–µ—Ç —ç—Ç–æ —Å–∞–º

        return x, trans, trans_feat

# ============================================================================
# –ë–õ–û–ö 6: –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê –°–û –°–ú–ï–®–ê–ù–ù–´–ú–ò LABELS
# ============================================================================


    """
    –ú–û–î–ò–§–ò–¶–ò–†–û–í–ê–ù–ù–´–ô –î–ê–¢–ê–°–ï–¢ –¥–ª—è segmentation

    –ì–ª–∞–≤–Ω–æ–µ –æ—Ç–ª–∏—á–∏–µ:
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç labels –¥–ª—è –ö–ê–ñ–î–û–ô —Ç–æ—á–∫–∏ (num_points,)
    - –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç majority voting
    """

    def __init__(self, folder_path, num_points=2048, augment=True):
        """
        Args:
            folder_path: –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å PLY —Ñ–∞–π–ª–∞–º–∏
            num_points: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ (2048)
            augment: –ø—Ä–∏–º–µ–Ω—è—Ç—å –ª–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é
        """
        self.num_points = num_points
        self.augment = augment

        print(f"\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è segmentation:")

        self.files = glob.glob(os.path.join(folder_path, "*.ply"))

        if len(self.files) == 0:
            print(f"‚ùå –ù–µ—Ç .ply —Ñ–∞–π–ª–æ–≤ –≤ {folder_path}!")
        else:
            print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(self.files)} —Ñ–∞–π–ª–æ–≤")

        self.total_samples = len(self.files)

    def _read_ply_with_labels(self, file_path):
        """
        –ß–∏—Ç–∞–µ—Ç .ply —Ñ–∞–π–ª —Å labels –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏

        Returns:
            tuple: (points, labels) –∏–ª–∏ (None, None)
        """
        try:
            plydata = PlyData.read(file_path)
            vertex = plydata['vertex']

            # –ß—Ç–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
            points = np.vstack([vertex['x'], vertex['y'], vertex['z']]).T

            # –ß—Ç–µ–Ω–∏–µ labels
            labels = None
            for label_field in ['label', 'class', 'classification', 'scalar_label']:
                if label_field in vertex.dtype.names:
                    labels = np.array(vertex[label_field])
                    break

            if labels is None:
                print(f"‚ö†Ô∏è –§–∞–π–ª {os.path.basename(file_path)} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª—è label!")
                return None, None

            # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è labels
            labels = (labels == 1).astype(np.int64)

            return points, labels

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {os.path.basename(file_path)}: {e}")
            return None, None

    def __len__(self):
        return self.total_samples

    def __getitem__(self, idx):
        """
        –ú–û–î–ò–§–ò–¶–ò–†–û–í–ê–ù –¥–ª—è segmentation

        Returns:
            tuple:
                - points: (3, num_points) ‚Äî –æ–±–ª–∞–∫–æ —Ç–æ—á–µ–∫
                - labels: (num_points,) ‚Äî label –¥–ª—è –ö–ê–ñ–î–û–ô —Ç–æ—á–∫–∏
        """
        file_path = self.files[idx]
        points, labels = self._read_ply_with_labels(file_path)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
        if points is None or labels is None or len(points) == 0:
            points = np.random.randn(self.num_points, 3).astype(np.float32)
            labels = np.zeros(self.num_points, dtype=np.int64)

        # === –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø ===
        centroid = np.mean(points, axis=0)
        points = points - centroid

        m = np.max(np.sqrt(np.sum(points**2, axis=1)))
        if m > 1e-8:
            points = points / m

        # === –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–Ø ===
        if self.augment:
            # –ü–æ–≤–æ—Ä–æ—Ç
            theta = np.random.uniform(0, 2 * np.pi)
            cos_t, sin_t = np.cos(theta), np.sin(theta)
            rotation = np.array([
                [cos_t, -sin_t, 0],
                [sin_t, cos_t, 0],
                [0, 0, 1]
            ], dtype=np.float32)
            points = points @ rotation.T

            # –ú–∞—Å—à—Ç–∞–±
            points *= np.random.uniform(0.8, 1.2)

            # –®—É–º
            points += np.random.normal(0, 0.02, points.shape).astype(np.float32)

        # === RESAMPLING ===
        if len(points) >= self.num_points:
            idx_sample = np.random.choice(len(points), self.num_points, replace=False)
        else:
            idx_sample = np.random.choice(len(points), self.num_points, replace=True)

        points = points[idx_sample]
        labels = labels[idx_sample]

        # === –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –í –¢–ï–ù–ó–û–† ===
        points_tensor = torch.from_numpy(points.T.copy()).float()  # (3, num_points)
        labels_tensor = torch.from_numpy(labels.copy()).long()      # (num_points,)

        # –ù–û–í–û–ï: –í–æ–∑–≤—Ä–∞—â–∞–µ–º labels –¥–ª—è –ö–ê–ñ–î–û–ô —Ç–æ—á–∫–∏, –Ω–µ majority label
        return points_tensor, labels_tensor
    
class SegmentationDataset(Dataset):
    """
    –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è point-wise segmentation.

    –ì–ª–∞–≤–Ω–æ–µ:
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç labels –¥–ª—è –ö–ê–ñ–î–û–ô —Ç–æ—á–∫–∏ (num_points,)
    - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ —á–∏—Ç–∞–µ—Ç –ø–æ–ª–µ label –∏–∑ PLY —á–µ—Ä–µ–∑ vertex.data.dtype.names
    """

    def __init__(self, folder_path, num_points=2048, augment=True):
        self.num_points = num_points
        self.augment = augment

        print(f"\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è segmentation:")

        self.files = glob.glob(os.path.join(folder_path, "*.ply"))

        if len(self.files) == 0:
            print(f"‚ùå –ù–µ—Ç .ply —Ñ–∞–π–ª–æ–≤ –≤ {folder_path}!")
        else:
            print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(self.files)} —Ñ–∞–π–ª–æ–≤")

        self.total_samples = len(self.files)

    def _read_ply_with_labels(self, file_path):
        """
        –ß–∏—Ç–∞–µ—Ç .ply —Ñ–∞–π–ª —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ –∏ labels –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            points: np.ndarray (N, 3)
            labels: np.ndarray (N,)  (0/1)
        """
        try:
            # –í–∞–∂–Ω–æ: PlyData.read –º–æ–∂–Ω–æ –≤—ã–∑—ã–≤–∞—Ç—å –ø—Ä—è–º–æ –ø–æ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É
            plydata = PlyData.read(file_path)
            vertex = plydata['vertex']          # PlyElement
            data = vertex.data                  # np.recarray —Å –ø–æ–ª—è–º–∏

            # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
            points = np.vstack([
                data['x'],
                data['y'],
                data['z']
            ]).T.astype(np.float32)

            # –ò–º–µ–Ω–∞ –≤—Å–µ—Ö –ø–æ–ª–µ–π –∏–∑ vertex.data
            field_names = data.dtype.names

            # –ü–æ–∏—Å–∫ –ø–æ–ª—è —Å –º–µ—Ç–∫–∞–º–∏
            labels = None
            for label_field in ['label', 'class', 'classification', 'scalar_label']:
                if label_field in field_names:
                    labels = np.array(data[label_field], dtype=np.int64)
                    break

            if labels is None:
                print(f"‚ö†Ô∏è –§–∞–π–ª {os.path.basename(file_path)} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª—è label!")
                return None, None

            # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è: –∑–¥–∞–Ω–∏—è/—Å—Ç–µ–Ω—ã = 1, –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ = 0
            labels = (labels == 1).astype(np.int64)

            # –ó–∞—â–∏—Ç–∞ –æ—Ç —Ä–∞—Å—Å–∏–Ω—Ö—Ä–æ–Ω–∞ –¥–ª–∏–Ω
            if len(points) != len(labels):
                print(f"‚ö†Ô∏è {os.path.basename(file_path)}: points({len(points)}) != labels({len(labels)})")
                n = min(len(points), len(labels))
                points = points[:n]
                labels = labels[:n]

            return points, labels

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {os.path.basename(file_path)}: {e}")
            return None, None

    def __len__(self):
        return self.total_samples

    def __getitem__(self, idx):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            points: (3, num_points)
            labels: (num_points,)
        """
        file_path = self.files[idx]
        points, labels = self._read_ply_with_labels(file_path)

        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–≥–ª—É—à–∫—É
        if points is None or labels is None or len(points) == 0:
            points = np.random.randn(self.num_points, 3).astype(np.float32)
            labels = np.zeros(self.num_points, dtype=np.int64)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        centroid = np.mean(points, axis=0)
        points = points - centroid

        m = np.max(np.sqrt(np.sum(points ** 2, axis=1)))
        if m > 1e-8:
            points = points / m

        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        if self.augment:
            # –ü–æ–≤–æ—Ä–æ—Ç –≤–æ–∫—Ä—É–≥ Z
            theta = np.random.uniform(0, 2 * np.pi)
            cos_t, sin_t = np.cos(theta), np.sin(theta)
            rotation = np.array([
                [cos_t, -sin_t, 0],
                [sin_t,  cos_t, 0],
                [0,      0,     1]
            ], dtype=np.float32)
            points = points @ rotation.T

            # –ú–∞—Å—à—Ç–∞–±
            points *= np.random.uniform(0.8, 1.2)

            # –®—É–º
            points += np.random.normal(0, 0.02, points.shape).astype(np.float32)

        # Resampling
        if len(points) >= self.num_points:
            idx_sample = np.random.choice(len(points), self.num_points, replace=False)
        else:
            idx_sample = np.random.choice(len(points), self.num_points, replace=True)

        points = points[idx_sample]
        labels = labels[idx_sample]

        # –í —Ç–µ–Ω–∑–æ—Ä—ã
        points_tensor = torch.from_numpy(points.T.copy()).float()  # (3, num_points)
        labels_tensor = torch.from_numpy(labels.copy()).long()     # (num_points,)

        return points_tensor, labels_tensor

# ============================================================================
# –ë–õ–û–ö 7: –§–£–ù–ö–¶–ò–ò –û–ë–£–ß–ï–ù–ò–Ø –ò –í–ê–õ–ò–î–ê–¶–ò–ò
# ============================================================================

def train_epoch(model, loader, optimizer, criterion, device, use_amp, epoch):
    """
    –û–¥–Ω–∞ —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è segmentation

    –ò–ó–ú–ï–ù–ï–ù–û: –º–µ—Ç—Ä–∏–∫–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è –¥–ª—è –ö–ê–ñ–î–û–ô —Ç–æ—á–∫–∏
    """
    model.train()

    total_loss, correct, total = 0.0, 0, 0
    scaler = torch.amp.GradScaler('cuda') if use_amp else None

    pbar = tqdm(loader, desc=f"Epoch {epoch+1} Train")

    for points, labels in pbar:
        # points: (batch, 3, num_points)
        # labels: (batch, num_points)

        points, labels = points.to(device), labels.to(device)
        optimizer.zero_grad(set_to_none=True)

        if use_amp:
            with torch.cuda.amp.autocast(enabled=True):
                outputs, trans, trans_feat = model(points)
                # outputs: (batch, num_classes, num_points)

                # –ù–û–í–û–ï: Loss —Å—á–∏—Ç–∞–µ—Ç—Å—è –¥–ª—è –ö–ê–ñ–î–û–ô —Ç–æ—á–∫–∏
                # –ù—É–∂–Ω–æ –ø–µ—Ä–µ—Å—Ç–∞–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è CrossEntropyLoss
                outputs = outputs.transpose(2, 1).contiguous()  # (batch, num_points, num_classes)
                outputs = outputs.view(-1, outputs.size(-1))     # (batch*num_points, num_classes)
                labels_flat = labels.view(-1)                    # (batch*num_points,)

                loss = criterion(outputs, labels_flat)

                # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
                if trans_feat is not None:
                    k = trans_feat.size(1)
                    I = torch.eye(k, device=device).unsqueeze(0).repeat(trans_feat.size(0), 1, 1)
                    reg_loss = F.mse_loss(torch.bmm(trans_feat, trans_feat.transpose(2, 1)), I)
                    loss = loss + 0.001 * reg_loss

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

        else:
            outputs, trans, trans_feat = model(points)
            outputs = outputs.transpose(2, 1).contiguous()
            outputs = outputs.view(-1, outputs.size(-1))
            labels_flat = labels.view(-1)

            loss = criterion(outputs, labels_flat)

            if trans_feat is not None:
                k = trans_feat.size(1)
                I = torch.eye(k, device=device).unsqueeze(0).repeat(trans_feat.size(0), 1, 1)
                reg_loss = F.mse_loss(torch.bmm(trans_feat, trans_feat.transpose(2, 1)), I)
                loss = loss + 0.001 * reg_loss

            loss.backward()
            optimizer.step()

        # === –ú–ï–¢–†–ò–ö–ò ===
        _, pred = torch.max(outputs, 1)  # (batch*num_points,)

        total += labels_flat.size(0)  # –í—Å–µ–≥–æ —Ç–æ—á–µ–∫
        correct += (pred == labels_flat).sum().item()  # –ü—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫

        total_loss += loss.item()

        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})

    return total_loss / len(loader), 100 * correct / total


def validate_epoch(model, loader, criterion, device, epoch):
    """
    –û–¥–Ω–∞ —ç–ø–æ—Ö–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è segmentation
    """
    model.eval()

    total_loss, correct, total = 0.0, 0, 0

    with torch.no_grad():
        pbar = tqdm(loader, desc=f"Epoch {epoch+1} Val")

        for points, labels in pbar:
            points, labels = points.to(device), labels.to(device)

            outputs, _, _ = model(points)

            outputs = outputs.transpose(2, 1).contiguous()
            outputs = outputs.view(-1, outputs.size(-1))
            labels_flat = labels.view(-1)

            loss = criterion(outputs, labels_flat)

            _, pred = torch.max(outputs, 1)

            total += labels_flat.size(0)
            correct += (pred == labels_flat).sum().item()

            total_loss += loss.item()

            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})

    return total_loss / len(loader), 100 * correct / total

# ============================================================================
# –ë–õ–û–ö 8: –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================================================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è PointNet Segmentation"""

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
    if not os.path.exists(MIXED_DATA_FOLDER):
        print(f"‚ùå {MIXED_DATA_FOLDER} –Ω–µ –Ω–∞–π–¥–µ–Ω!"); return

    print(f"\n‚úÖ –ü–∞–ø–∫–∞ —Å –¥–∞–Ω–Ω—ã–º–∏: {MIXED_DATA_FOLDER}/")

    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset = SegmentationDataset(MIXED_DATA_FOLDER, NUM_POINTS, True)

    if dataset.total_samples == 0:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö!"); return

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ train/val
    train_size = int((1-VAL_SPLIT)*len(dataset))
    train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, len(dataset)-train_size])

    # DataLoaders
    train_loader = DataLoader(
        train_ds, BATCH_SIZE, True,
        num_workers=NUM_WORKERS, pin_memory=True
    )

    val_loader = DataLoader(
        val_ds, BATCH_SIZE, False,
        num_workers=NUM_WORKERS, pin_memory=True
    )

    # –ù–û–í–ê–Ø –ú–û–î–ï–õ–¨: PointNet Segmentation
    model = PointNetSegmentation(2).to(device)
    print(f"\nüß† –ú–æ–¥–µ–ª—å: PointNet Segmentation (point-wise classification)")

    # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â—É—é –º–æ–¥–µ–ª—å
    if os.path.exists(BASE_MODEL):
        try:
            state = torch.load(BASE_MODEL, map_location=device)
            if isinstance(state, dict) and 'state_dict' in state:
                model.load_state_dict(state['state_dict'])
            else:
                model.load_state_dict(state)
            print(f"\n{'='*80}\nüîÑ –î–û–û–ë–£–ß–ï–ù–ò–ï: {BASE_MODEL} –∑–∞–≥—Ä—É–∂–µ–Ω\n{'='*80}")
        except Exception as e:
            print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    else:
        print(f"\n‚ö†Ô∏è {BASE_MODEL} –Ω–µ –Ω–∞–π–¥–µ–Ω - –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")

    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
    criterion = nn.CrossEntropyLoss()

    best_acc = 0.0
    start = time.time()

    print(f"\nüöÄ –ù–ê–ß–ê–õ–û: {len(train_ds)} train, {len(val_ds)} val\n")

    # –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª —ç–ø–æ—Ö
    for epoch in range(NUM_EPOCHS):
        print(f"\n{'‚îÄ'*80}\nEpoch {epoch+1}/{NUM_EPOCHS}\n{'‚îÄ'*80}")

        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device, USE_MIXED_PRECISION, epoch)
        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device, epoch)

        scheduler.step()

        print(f"\nüìä Train: {train_loss:.4f}, {train_acc:.2f}% | Val: {val_loss:.4f}, {val_acc:.2f}%")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), 'best_model_segmentation.pth')
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ! Best: {best_acc:.2f}%")

    elapsed = (time.time() - start) / 60

    print(f"\n{'='*80}\n‚úÖ –ó–ê–í–ï–†–®–ï–ù–û: {best_acc:.2f}%, {elapsed:.1f}–º–∏–Ω\n{'='*80}")
    print(f"\nüìÅ best_model_segmentation.pth")
    print(f"\nüéØ –¢–µ–ø–µ—Ä—å –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ö–ê–ñ–î–£–Æ —Ç–æ—á–∫—É –æ—Ç–¥–µ–ª—å–Ω–æ!")
    print(f"   –í–º–µ—Å—Ç–æ: –æ–±–ª–∞–∫–æ ‚Üí –∑–¥–∞–Ω–∏–µ/–Ω–µ-–∑–¥–∞–Ω–∏–µ")
    print(f"   –¢–µ–ø–µ—Ä—å: –∫–∞–∂–¥–∞—è —Ç–æ—á–∫–∞ ‚Üí –∑–¥–∞–Ω–∏–µ/–Ω–µ-–∑–¥–∞–Ω–∏–µ")

    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# ============================================================================
# –ë–õ–û–ö 9: –¢–û–ß–ö–ê –í–•–û–î–ê
# ============================================================================

if __name__ == "__main__":
    main()
