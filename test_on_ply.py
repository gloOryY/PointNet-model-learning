"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π PointNet Segmentation –Ω–∞ —Ñ–∞–π–ª–µ data.ply
–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ö–ê–ñ–î–£–Æ —Ç–æ—á–∫—É –∫–∞–∫ –∑–¥–∞–Ω–∏–µ (1) / –Ω–µ-–∑–¥–∞–Ω–∏–µ (0)
+ –†–ê–°–ß–ï–¢ –ú–ï–¢–†–ò–ö –ö–ê–ß–ï–°–¢–í–ê (–µ—Å–ª–∏ –µ—Å—Ç—å ground truth)
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from plyfile import PlyData, PlyElement  # ‚Üê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —á—Ç–µ–Ω–∏—è –ò –∑–∞–ø–∏—Å–∏
from tqdm import tqdm
import time


print("=" * 80)
print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï POINTNET SEGMENTATION –ù–ê DATA.PLY")
print("=" * 80)


# === –ü–ê–†–ê–ú–ï–¢–†–´ ===
INPUT_FILE = "test.ply"  # ‚Üê –ò–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ .ply
OUTPUT_FILE = "data_classified.ply"
MODEL_PATH = "best_model_segmentation.pth"
NUM_POINTS = 2048
BATCH_SIZE = 32


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}")


# ============================================================================
# –ê–†–•–ò–¢–ï–ö–¢–£–†–ê (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
# ============================================================================

class TNet(nn.Module):
    def __init__(self, k=3):
        super(TNet, self).__init__()
        self.k = k
        self.conv1 = nn.Conv1d(k,   64, 1)
        self.conv2 = nn.Conv1d(64, 128, 1)
        self.conv3 = nn.Conv1d(128,1024, 1)
        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512,  256)
        self.fc3 = nn.Linear(256,  k*k)
        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(1024)
        self.bn4 = nn.BatchNorm1d(512)
        self.bn5 = nn.BatchNorm1d(256)

    def forward(self, x):
        batch_size = x.size(0)
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))
        x = torch.max(x, 2, keepdim=True)[0]
        x = x.view(batch_size, -1)
        x = F.relu(self.bn4(self.fc1(x)))
        x = F.relu(self.bn5(self.fc2(x)))
        x = self.fc3(x)
        identity = torch.eye(self.k, device=x.device).flatten()
        x = x + identity
        x = x.view(batch_size, self.k, self.k)
        return x


class PointNetSegmentation(nn.Module):
    def __init__(self, num_classes=2):
        super(PointNetSegmentation, self).__init__()
        self.input_transform = TNet(k=3)
        self.conv1 = nn.Conv1d(3,   64, 1)
        self.conv2 = nn.Conv1d(64, 128, 1)
        self.conv3 = nn.Conv1d(128,128, 1)
        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(128)
        self.feature_transform = TNet(k=64)
        self.conv4 = nn.Conv1d(128, 512,  1)
        self.conv5 = nn.Conv1d(512, 2048, 1)
        self.bn4 = nn.BatchNorm1d(512)
        self.bn5 = nn.BatchNorm1d(2048)
        self.conv6 = nn.Conv1d(2112, 512, 1)
        self.conv7 = nn.Conv1d(512,  256, 1)
        self.conv8 = nn.Conv1d(256,  128, 1)
        self.conv9 = nn.Conv1d(128,  num_classes, 1)
        self.bn6 = nn.BatchNorm1d(512)
        self.bn7 = nn.BatchNorm1d(256)
        self.bn8 = nn.BatchNorm1d(128)
        self.dropout = nn.Dropout(p=0.3)

    def forward(self, x):
        batch_size = x.size(0)
        num_points = x.size(2)
        trans = self.input_transform(x)
        x = torch.bmm(trans, x)
        x = F.relu(self.bn1(self.conv1(x)))
        local_features = x
        trans_feat = self.feature_transform(x)
        x = x.transpose(2, 1)
        x = torch.bmm(x, trans_feat)
        x = x.transpose(2, 1)
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))
        x = F.relu(self.bn4(self.conv4(x)))
        x = F.relu(self.bn5(self.conv5(x)))
        global_features = torch.max(x, 2, keepdim=True)[0]
        global_features = global_features.repeat(1, 1, num_points)
        x = torch.cat([local_features, global_features], dim=1)
        x = F.relu(self.bn6(self.conv6(x)))
        x = self.dropout(x)
        x = F.relu(self.bn7(self.conv7(x)))
        x = self.dropout(x)
        x = F.relu(self.bn8(self.conv8(x)))
        x = self.conv9(x)
        return x, trans, trans_feat


# ============================================================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ============================================================================

def normalize_point_cloud(points):
    centroid = np.mean(points, axis=0)
    points = points - centroid
    m = np.max(np.sqrt(np.sum(points**2, axis=1)))
    points = points / (m + 1e-8)
    return points


def segment_full_cloud(model, points, num_points, batch_size):
    N = len(points)
    labels = np.zeros(N, dtype=np.int32)
    confidences = np.zeros(N, dtype=np.float32)
    
    chunks = []
    for start in range(0, N, num_points):
        end = min(start + num_points, N)
        if end - start == 0:
            continue
        chunks.append((start, end))
    
    print(f"\n–í—Å–µ–≥–æ —Ç–æ—á–µ–∫: {N:,}")
    print(f"–ß–∞–Ω–∫ —Ä–∞–∑–º–µ—Ä–æ–º: {num_points}, –≤—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
    
    model.eval()
    
    with torch.no_grad():
        for batch_start in tqdm(range(0, len(chunks), batch_size), desc="–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è"):
            batch_end = min(batch_start + batch_size, len(chunks))
            batch_chunks = chunks[batch_start:batch_end]
            
            batch_points = []
            batch_meta = []
            
            for (s, e) in batch_chunks:
                chunk_pts = points[s:e]
                chunk_norm = normalize_point_cloud(chunk_pts.copy())
                current = len(chunk_norm)
                if current < num_points:
                    idx = np.random.choice(current, num_points, replace=True)
                else:
                    idx = np.arange(current)[:num_points]
                
                chunk_resampled = chunk_norm[idx]
                batch_points.append(chunk_resampled)
                batch_meta.append((s, e, current))
            
            batch_tensor = torch.FloatTensor(np.array(batch_points)).to(device)
            batch_tensor = batch_tensor.transpose(2, 1).contiguous()
            
            logits, _, _ = model(batch_tensor)
            probs = F.softmax(logits, dim=1)
            preds = torch.argmax(probs, dim=1)
            confs, _ = torch.max(probs, dim=1)
            
            preds_cpu = preds.cpu().numpy()
            confs_cpu = confs.cpu().numpy()
            
            for i, (s, e, real_n) in enumerate(batch_meta):
                labels[s:e] = preds_cpu[i, :real_n]
                confidences[s:e] = confs_cpu[i, :real_n]
    
    return labels, confidences


def calculate_metrics(gt_labels, pred_labels):
    """
    –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    
    –ú–µ—Ç—Ä–∏–∫–∏:
    - Accuracy: –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫
    - Precision: TP / (TP + FP) - —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–∞ "–∑–¥–∞–Ω–∏–µ"
    - Recall: TP / (TP + FN) - –ø–æ–ª–Ω–æ—Ç–∞ –∑–∞—Ö–≤–∞—Ç–∞ –∫–ª–∞—Å—Å–∞ "–∑–¥–∞–Ω–∏–µ"
    - F1-Score: –≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ Precision –∏ Recall
    - IoU (Intersection over Union): TP / (TP + FP + FN)
    """
    tp = np.sum((pred_labels == 1) & (gt_labels == 1))
    fp = np.sum((pred_labels == 1) & (gt_labels == 0))
    tn = np.sum((pred_labels == 0) & (gt_labels == 0))
    fn = np.sum((pred_labels == 0) & (gt_labels == 1))
    
    accuracy = (tp + tn) / (tp + fp + tn + fn) if (tp + fp + tn + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'iou': iou,
        'tp': tp,
        'fp': fp,
        'tn': tn,
        'fn': fn
    }


def save_classified_cloud(points, labels, confidences, output_file):
    colors = np.zeros((len(points), 3), dtype=np.uint8)
    colors[labels == 1] = [0, 255, 0]  # –ó–µ–ª—ë–Ω—ã–π = –∑–¥–∞–Ω–∏–µ
    colors[labels == 0] = [255, 0, 0]  # –ö—Ä–∞—Å–Ω—ã–π = –Ω–µ-–∑–¥–∞–Ω–∏–µ
    
    vertex = np.array(
        [
            (points[i, 0], points[i, 1], points[i, 2],
             colors[i, 0], colors[i, 1], colors[i, 2],
             int(labels[i]), float(confidences[i]))
            for i in range(len(points))
        ],
        dtype=[
            ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),
            ('red', 'u1'), ('green', 'u1'), ('blue', 'u1'),
            ('label', 'i4'), ('confidence', 'f4')
        ]
    )
    
    el = PlyElement.describe(vertex, 'vertex')
    PlyData([el]).write(output_file)


# ============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================================================

def main():
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (segmentation): {MODEL_PATH}")
    model = PointNetSegmentation(num_classes=2).to(device)
    
    try:
        state_dict = torch.load(MODEL_PATH, map_location=device)
        model.load_state_dict(state_dict)
        print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return
    
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ PLY —Ñ–∞–π–ª–∞
    print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫: {INPUT_FILE}")
    try:
        # ‚Üê –ò–ó–ú–ï–ù–ï–ù–û: –ß—Ç–µ–Ω–∏–µ PLY –≤–º–µ—Å—Ç–æ LAZ
        plydata = PlyData.read(INPUT_FILE)
        vertex_data = plydata['vertex']
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç X, Y, Z
        points = np.vstack([
            vertex_data['x'],
            vertex_data['y'],
            vertex_data['z']
        ]).T.astype(np.float32)
        
        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(points):,} —Ç–æ—á–µ–∫")
        print(f"  X: [{points[:, 0].min():.2f}, {points[:, 0].max():.2f}]")
        print(f"  Y: [{points[:, 1].min():.2f}, {points[:, 1].max():.2f}]")
        print(f"  Z: [{points[:, 2].min():.2f}, {points[:, 2].max():.2f}]")
        
        # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å ground truth labels –∏–∑ PLY
        gt_labels = None
        possible_label_fields = ['label', 'classification', 'class', 'scalar_Classification']
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–æ–ª–µ–π
        available_fields = vertex_data.data.dtype.names
        print(f"\n  –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–ª—è –≤ PLY: {available_fields}")
        
        for field_name in possible_label_fields:
            if field_name in available_fields:
                gt_labels = np.array(vertex_data[field_name], dtype=np.int32)
                # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è: –∑–¥–∞–Ω–∏—è = 1, –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ = 0
                gt_labels = (gt_labels == 1).astype(np.int32)
                print(f"‚úì –ù–∞–π–¥–µ–Ω–æ –ø–æ–ª–µ ground truth: {field_name}")
                print(f"  –ó–¥–∞–Ω–∏—è: {np.sum(gt_labels == 1):,} ({100*np.sum(gt_labels == 1)/len(gt_labels):.1f}%)")
                print(f"  –ù–µ-–∑–¥–∞–Ω–∏—è: {np.sum(gt_labels == 0):,} ({100*np.sum(gt_labels == 0)/len(gt_labels):.1f}%)")
                break
        
        if gt_labels is None:
            print("‚ö†Ô∏è  Ground truth –Ω–µ –Ω–∞–π–¥–µ–Ω - –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –±—É–¥—É—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã")
            
    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 3. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤—Å–µ—Ö —Ç–æ—á–µ–∫
    print("\n–ù–∞—á–∞–ª–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤—Å–µ–≥–æ –æ–±–ª–∞–∫–∞...")
    start_time = time.time()
    
    pred_labels, confidences = segment_full_cloud(
        model, points, NUM_POINTS, BATCH_SIZE
    )
    
    elapsed_time = time.time() - start_time
    
    # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    num_buildings = np.sum(pred_labels == 1)
    num_non_buildings = np.sum(pred_labels == 0)
    avg_confidence = float(np.mean(confidences))
    
    print("\n" + "=" * 80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–ò")
    print("=" * 80)
    print(f"\n–í—Å–µ–≥–æ —Ç–æ—á–µ–∫: {len(points):,}")
    print(f"–ó–¥–∞–Ω–∏—è (–∑–µ–ª—ë–Ω—ã–π): {num_buildings:,} ({100*num_buildings/len(points):.1f}%)")
    print(f"–ù–µ-–∑–¥–∞–Ω–∏—è (–∫—Ä–∞—Å–Ω—ã–π): {num_non_buildings:,} ({100*num_non_buildings/len(points):.1f}%)")
    print(f"–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.2%}")
    print(f"–í—Ä–µ–º—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {elapsed_time:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"–°–∫–æ—Ä–æ—Å—Ç—å: {len(points)/elapsed_time:.0f} —Ç–æ—á–µ–∫/—Å–µ–∫")
    
    # 5. –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
    if gt_labels is not None:
        print("\n" + "=" * 80)
        print("–ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê –î–ï–¢–ï–ö–¶–ò–ò")
        print("=" * 80)
        
        metrics = calculate_metrics(gt_labels, pred_labels)
        
        print(f"\nüìä –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
        print(f"  ‚Ä¢ Accuracy (—Ç–æ—á–Ω–æ—Å—Ç—å):  {metrics['accuracy']:.2%}")
        print(f"    ‚Üí –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫")
        print(f"    ‚Üí –§–æ—Ä–º—É–ª–∞: (TP + TN) / –í—Å–µ–≥–æ —Ç–æ—á–µ–∫")
        
        print(f"\n  ‚Ä¢ Precision (—Ç–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ '–∑–¥–∞–Ω–∏–µ'):  {metrics['precision']:.2%}")
        print(f"    ‚Üí –ò–∑ –≤—Å–µ—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–¥–∞–Ω–∏–π - —Å–∫–æ–ª—å–∫–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–¥–∞–Ω–∏—è")
        print(f"    ‚Üí –§–æ—Ä–º—É–ª–∞: TP / (TP + FP)")
        
        print(f"\n  ‚Ä¢ Recall (–ø–æ–ª–Ω–æ—Ç–∞ –∫–ª–∞—Å—Å–∞ '–∑–¥–∞–Ω–∏–µ'):  {metrics['recall']:.2%}")
        print(f"    ‚Üí –ò–∑ –≤—Å–µ—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–¥–∞–Ω–∏–π - —Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å –Ω–∞—à–ª–∞")
        print(f"    ‚Üí –§–æ—Ä–º—É–ª–∞: TP / (TP + FN)")
        
        print(f"\n  ‚Ä¢ F1-Score (–≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ):  {metrics['f1_score']:.2%}")
        print(f"    ‚Üí –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É Precision –∏ Recall")
        print(f"    ‚Üí –§–æ—Ä–º—É–ª–∞: 2 √ó (Precision √ó Recall) / (Precision + Recall)")
        
        print(f"\n  ‚Ä¢ IoU (Intersection over Union):  {metrics['iou']:.2%}")
        print(f"    ‚Üí –ü–ª–æ—â–∞–¥—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è / –ü–ª–æ—â–∞–¥—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è")
        print(f"    ‚Üí –§–æ—Ä–º—É–ª–∞: TP / (TP + FP + FN)")
        
        print(f"\nüìà Confusion Matrix:")
        print(f"  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
        print(f"  ‚îÇ                 ‚îÇ  Pred: –ó–¥–∞–Ω–∏–µ‚îÇ Pred: –ù–µ-–∑–¥–∞–Ω‚îÇ")
        print(f"  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")
        print(f"  ‚îÇ GT: –ó–¥–∞–Ω–∏–µ      ‚îÇ {metrics['tp']:>12,} ‚îÇ {metrics['fn']:>12,} ‚îÇ")
        print(f"  ‚îÇ GT: –ù–µ-–∑–¥–∞–Ω–∏–µ   ‚îÇ {metrics['fp']:>12,} ‚îÇ {metrics['tn']:>12,} ‚îÇ")
        print(f"  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")
        
        print(f"\n  TP (True Positive):  {metrics['tp']:,} - –ó–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –∫–∞–∫ –∑–¥–∞–Ω–∏–µ ‚úì")
        print(f"  TN (True Negative):  {metrics['tn']:,} - –ù–µ-–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –∫–∞–∫ –Ω–µ-–∑–¥–∞–Ω–∏–µ ‚úì")
        print(f"  FP (False Positive): {metrics['fp']:,} - –ù–µ-–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –∫–∞–∫ –∑–¥–∞–Ω–∏–µ ‚úó")
        print(f"  FN (False Negative): {metrics['fn']:,} - –ó–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –∫–∞–∫ –Ω–µ-–∑–¥–∞–Ω–∏–µ ‚úó")
    
    # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    print(f"\n{'='*80}")
    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {OUTPUT_FILE}")
    save_classified_cloud(points, pred_labels, confidences, OUTPUT_FILE)
    print("‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω")
    
    print("\n" + "=" * 80)
    print("–ó–ê–í–ï–†–®–ï–ù–û")
    print("=" * 80)
    print(f"\n–û—Ç–∫—Ä–æ–π —Ñ–∞–π–ª {OUTPUT_FILE} –≤ CloudCompare –∏–ª–∏ –¥—Ä—É–≥–æ–º PLY viewer")
    print("–¶–≤–µ—Ç–∞:")
    print("  üü¢ –ó–µ–ª—ë–Ω—ã–π = –ó–¥–∞–Ω–∏—è/—Å—Ç–µ–Ω—ã (label=1)")
    print("  üî¥ –ö—Ä–∞—Å–Ω—ã–π = –§–æ–Ω, –Ω–µ-–∑–¥–∞–Ω–∏—è (label=0)")


if __name__ == "__main__":
    main()
