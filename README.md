# PointNet-model-learning (accuracy 47%)
Этот проект предназначен для обучения и тестирования нейросети PointNet для бинарной классификации точек в облаках LIDAR: "здание" (1) и "не-здание/фон" (0). В процессе используется архитектура PointNet c двумя трансформационными сетями (TNet), а также комплексы процедур по подготовке, аугментации, обучению и тестированию на ваших данных.

Структура файлов
finetune_model.py — дообучение (fine-tuning) PointNet на подготовленном датасете для Tesla T4.

test_on_data_laz.py — тестирование обученной PointNet на облаке точек из файла data.laz и сохранение результата в PLY.

Теория: Архитектура PointNet
1. Базовые понятия
Облако точек: набор координат (x, y, z) предметов или поверхностей, полученных, например, с помощью LIDAR.

Бинарная классификация: задача, где каждая точка по свойствам и окружению классифицируется либо как "здание", либо "не-здание".

2. TNet (Transformation Net)
Зачем нужен?

PointNet не чувствителен к положению, но иногда точки во входных данных могут быть повернуты или масштабированы.

TNet учится находить матрицу преобразования, чтобы "выравнивать" облако для дальнейшей обработки.

Реализация:

Вход: облако точек размером 
(k,N), где обычно 
k=3 (координаты x, y, z).

Последовательность слоёв: Conv1d 
→ BatchNorm 
→ ReLU (выделяют признаки) 
→ MaxPool (агрегирует глобальный признак).

Далее полносвязные слои выводят матрицу размера 
k×k (например, 3x3 для коорд.).

Вводится регуляризация, заставляющая матрицу быть близкой к ортогональной .

3. PointNet — основная архитектура
Вход: нормированное облако точек.

Главные блоки:

Input Тransformation: TNet "выравнивает" координаты.

Conv1d + BatchNorm + ReLU: выделяют локальные признаки для каждой точки.

Feature Тransformation: TNet "выравнивает" полученные признаки (классификационная инвариантность).

Ещё Conv1d, BatchNorm: углубляют признаки.

Global Max Pooling: агрегирует все точки в один глобальный вектор — усиливаем устойчивость к перестановкам.

FC-слои (classifier): три последовательных слоя, Dropout для защиты от переобучения.

Output: два логита (до softmax), к которым применяется функция потерь CrossEntropyLoss впоследствии.

Важные моменты:

BatchNorm на каждом слое — ускоряет и стабилизирует обучение.

Dropout (p=0.3) — снижает переобучение.

Подготовка и работа с данными
1. Структура данных
Позитивные примеры (label=1): .ply-файлы зданий внутри ZIP (wall.zip)

Негативные примеры (label=0): .ply-файлы другого типа (фон/шум) в папке (non_buildings_ply/).

Классы уравниваются по количеству — это критично!

2. MixedDataset
Автоматически балансирует классы и подготавливает данные.

Операции для каждого облака:

Чтение (из архива или файлов)

Нормализация: центрирование и масштабирование облака в единичный шар

Аугментация (если задана):

Случайный поворот вокруг OZ

Случайный масштаб (от 0.8 до 1.2)

Добавление гауссового шума

Sampling: если точек слишком много/мало — выбирается 
N
=
2048
N=2048 случайных точек (с повторением или без)

Конвертация в torch.Tensor: для обработки нейросетью

Обучение модели (finetune_model.py)
Гиперпараметры
BATCH_SIZE = 32: размер батча

NUM_POINTS = 2048: количество точек в облаке

NUM_EPOCHS = 20: количество эпох обучения

LEARNING_RATE = 0.0001: шаг обновления весов

VAL_SPLIT = 0.2: доля валидации

USE_MIXED_PRECISION = True: смешанная точность ускоряет обучение на современных GPU.

Специфика процесса
Инициализация и загрузка файлов:

Проверка наличия данных

Загрузка и балансировка датасета

Разделение train/val:

train: 80%, val: 20%

DataLoader:

Параллелизм при загрузке

Pin memory — ускоряет передачу данных на GPU

Загрузка модели и весов:

Модель инициализируется с нуля или дообучается от весов best_model.pth

Оптимизатор Adam, CrossEntropyLoss, LR Scheduler

Цикл обучения/валидации:

После каждой эпохи актуализируется learning rate

Лучшая модель сохраняется в best_model_finetuned.pth

Dropout работает только во время обучения

Регуляризация ортогональности матриц TNet/Feature Transform

Тестирование (test_on_data_laz.py)
Загрузка модели и облака точек data.laz (LAS формат).

Скользящее окно:

Всё облако разбивается на окна по 2048
2048 точек с перекрытием stride=1024.

К каждому окну применяется нейросеть, окна агрегируются — "голосование" (prediction и confidence усредняются по окнам).

Нормализация каждого окна — обязательно, как при обучении!

Сохранение результата

Итоговые метки точек и уверенности сохраняются в цветной PLY (data_classified.ply):

здание: зеленый ([0][255][0])

не-здание: красный ([255][0][0])

Файл можно посмотреть в CloudCompare или любом PLY viewer.

Краткий пример сценария запуска
Соберите обучающий датасет (архив зданий + папка не-зданий, пример: 300+300 файлов).

Запустите дообучение:

python finetune_model.py
После 20 эпох лучшая модель сохранится.

Запустите тестирование:

python test_on_data_laz.py
Сохранится файл data_classified.ply, который можно визуализировать.

FAQ и рекомендации
Почему важна балансировка классов?
Чтобы модель не "запоминала" только здания/только фон (переобучение на один класс).

Зачем аугментация?
Для лучшей переносимости на новые данные, устойчивости к шуму, поворотам и масштабам.

Почему нормализация облаков?
Это необходимое условие для корректной работы сети — иначе она получает облака разного масштаба и расположения.

Можно ли менять NUM_POINTS/BATCH_SIZE?
Можно — но при изменениях лучше пересчитать размер окна/stride, чтобы не перегружать GPU.

Можно обучать на других классах?
Да, но потребуется изменить num_classes и датасет, а также классификационный слой.
